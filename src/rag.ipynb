{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import MeCab\n",
    "import faiss\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from ast import literal_eval\n",
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.vectorstores.utils import DistanceStrategy\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.schema import Document\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers.document_compressors import CrossEncoderReranker\n",
    "from langchain_community.cross_encoders import HuggingFaceCrossEncoder\n",
    "from langchain_huggingface import HuggingFaceEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mecab\n",
    "mecab = MeCab.Tagger()\n",
    "def extract_nouns(text):\n",
    "    try:\n",
    "        parsed = mecab.parse(text)\n",
    "        nouns = []\n",
    "        for line in parsed.splitlines():\n",
    "            if '\\t' in line:  # MeCab 출력에서 유효한 줄만 처리\n",
    "                word, feature = line.split('\\t')\n",
    "                if feature.startswith('NNG') or feature.startswith('NNP'):  # 보통명사, 고유명사\n",
    "                    nouns.append(word)\n",
    "        return nouns\n",
    "    except Exception as e:\n",
    "        print(f\"Error during MeCab parsing: {e}\")\n",
    "        return text  # 실패 시 원문 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(793, 3)\n",
      "(276, 3)\n",
      "(7807, 3)\n",
      "(1672, 3)\n",
      "(2303, 3)\n",
      "(2137, 3)\n",
      "(2432, 3)\n",
      "(3225, 3)\n",
      "(2231, 3)\n",
      "(2315, 3)\n",
      "(818, 3)\n"
     ]
    }
   ],
   "source": [
    "korean_history_term_merged = pd.read_csv('../../contest_baseline_code/data/rag/korean_history_term_merged.csv')\n",
    "korean_history_textbook_25_merged = pd.read_csv('../../contest_baseline_code/data/rag/korean_history_textbook_25_merged.csv')\n",
    "korean_webtext_edu = pd.read_csv('../../contest_baseline_code/data/rag/korean-webtext-edu.csv')\n",
    "korean_wikipedia_edu = pd.read_csv('../../contest_baseline_code/data/rag/korean-wikipedia-edu.csv')\n",
    "openstax_econ = pd.read_csv('../../contest_baseline_code/data/rag/openstax_econ.csv')\n",
    "openstax_poli = pd.read_csv('../../contest_baseline_code/data/rag/openstax_poli.csv')\n",
    "openstax_psych = pd.read_csv('../../contest_baseline_code/data/rag/openstax_psych.csv')\n",
    "openstax_us_hist = pd.read_csv('../../contest_baseline_code/data/rag/openstax_us_hist.csv')\n",
    "openstax_world_hist1 = pd.read_csv('../../contest_baseline_code/data/rag/openstax_world_hist1.csv')\n",
    "openstax_world_hist2 = pd.read_csv('../../contest_baseline_code/data/rag/openstax_world_hist2.csv')\n",
    "train_aug_qa_cleaned = pd.read_csv('../../contest_baseline_code/data/rag/train_aug_qa_cleaned.csv')\n",
    "\n",
    "print(korean_history_term_merged.shape)\n",
    "print(korean_history_textbook_25_merged.shape)\n",
    "print(korean_webtext_edu.shape)\n",
    "print(korean_wikipedia_edu.shape)\n",
    "print(openstax_econ.shape)\n",
    "print(openstax_poli.shape)\n",
    "print(openstax_psych.shape)\n",
    "print(openstax_us_hist.shape)\n",
    "print(openstax_world_hist1.shape)\n",
    "print(openstax_world_hist2.shape)\n",
    "print(train_aug_qa_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG Data Count: 26009\n",
      "filtered (len > 25) RAG Data Count: 25548\n"
     ]
    }
   ],
   "source": [
    "######################\n",
    "## RAG Data Loading ##\n",
    "######################\n",
    "rag_folder = \"../../contest_baseline_code/data/rag\"\n",
    "rag_files = glob(f\"{rag_folder}/*.csv\")\n",
    "\n",
    "# Concatenate RAG data\n",
    "rag_data_source = [pd.read_csv(file) for file in rag_files]\n",
    "rag_data = pd.concat(rag_data_source, axis=0, ignore_index=True)\n",
    "print(f\"RAG Data Count: {rag_data.shape[0]}\")\n",
    "\n",
    "# Use only documents with at least 25 characters\n",
    "rag_data = rag_data[rag_data.context.str.len() >= 25]\n",
    "print(f\"filtered (len > 25) RAG Data Count: {rag_data.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25548/25548 [00:00<00:00, 699342.65it/s]\n",
      "Created a chunk of size 619, which is longer than the specified 600\n",
      "Created a chunk of size 1172, which is longer than the specified 600\n",
      "Created a chunk of size 1113, which is longer than the specified 600\n",
      "Created a chunk of size 666, which is longer than the specified 600\n",
      "Created a chunk of size 3822, which is longer than the specified 600\n",
      "Created a chunk of size 676, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n",
      "Created a chunk of size 1005, which is longer than the specified 600\n",
      "Created a chunk of size 1105, which is longer than the specified 600\n",
      "Created a chunk of size 863, which is longer than the specified 600\n",
      "Created a chunk of size 700, which is longer than the specified 600\n",
      "Created a chunk of size 846, which is longer than the specified 600\n",
      "Created a chunk of size 641, which is longer than the specified 600\n",
      "Created a chunk of size 798, which is longer than the specified 600\n",
      "Created a chunk of size 800, which is longer than the specified 600\n",
      "Created a chunk of size 602, which is longer than the specified 600\n",
      "Created a chunk of size 654, which is longer than the specified 600\n",
      "Created a chunk of size 1360, which is longer than the specified 600\n",
      "Created a chunk of size 845, which is longer than the specified 600\n",
      "Created a chunk of size 649, which is longer than the specified 600\n",
      "Created a chunk of size 896, which is longer than the specified 600\n",
      "Created a chunk of size 1182, which is longer than the specified 600\n",
      "Created a chunk of size 1978, which is longer than the specified 600\n",
      "Created a chunk of size 1332, which is longer than the specified 600\n",
      "Created a chunk of size 936, which is longer than the specified 600\n",
      "Created a chunk of size 642, which is longer than the specified 600\n",
      "Created a chunk of size 789, which is longer than the specified 600\n",
      "Created a chunk of size 616, which is longer than the specified 600\n",
      "Created a chunk of size 1019, which is longer than the specified 600\n",
      "Created a chunk of size 743, which is longer than the specified 600\n",
      "Created a chunk of size 793, which is longer than the specified 600\n",
      "Created a chunk of size 704, which is longer than the specified 600\n",
      "Created a chunk of size 654, which is longer than the specified 600\n",
      "Created a chunk of size 670, which is longer than the specified 600\n",
      "Created a chunk of size 635, which is longer than the specified 600\n",
      "Created a chunk of size 676, which is longer than the specified 600\n",
      "Created a chunk of size 786, which is longer than the specified 600\n",
      "Created a chunk of size 798, which is longer than the specified 600\n",
      "Created a chunk of size 834, which is longer than the specified 600\n",
      "Created a chunk of size 750, which is longer than the specified 600\n",
      "Created a chunk of size 3274, which is longer than the specified 600\n",
      "Created a chunk of size 1242, which is longer than the specified 600\n",
      "Created a chunk of size 786, which is longer than the specified 600\n",
      "Created a chunk of size 650, which is longer than the specified 600\n",
      "Created a chunk of size 691, which is longer than the specified 600\n",
      "Created a chunk of size 684, which is longer than the specified 600\n",
      "Created a chunk of size 881, which is longer than the specified 600\n",
      "Created a chunk of size 914, which is longer than the specified 600\n",
      "Created a chunk of size 1725, which is longer than the specified 600\n",
      "Created a chunk of size 710, which is longer than the specified 600\n",
      "Created a chunk of size 1186, which is longer than the specified 600\n",
      "Created a chunk of size 949, which is longer than the specified 600\n",
      "Created a chunk of size 607, which is longer than the specified 600\n",
      "Created a chunk of size 1117, which is longer than the specified 600\n",
      "Created a chunk of size 1015, which is longer than the specified 600\n",
      "Created a chunk of size 852, which is longer than the specified 600\n",
      "Created a chunk of size 837, which is longer than the specified 600\n",
      "Created a chunk of size 857, which is longer than the specified 600\n",
      "Created a chunk of size 1489, which is longer than the specified 600\n",
      "Created a chunk of size 869, which is longer than the specified 600\n",
      "Created a chunk of size 694, which is longer than the specified 600\n",
      "Created a chunk of size 884, which is longer than the specified 600\n",
      "Created a chunk of size 1198, which is longer than the specified 600\n",
      "Created a chunk of size 894, which is longer than the specified 600\n",
      "Created a chunk of size 662, which is longer than the specified 600\n",
      "Created a chunk of size 1025, which is longer than the specified 600\n",
      "Created a chunk of size 836, which is longer than the specified 600\n",
      "Created a chunk of size 762, which is longer than the specified 600\n",
      "Created a chunk of size 920, which is longer than the specified 600\n",
      "Created a chunk of size 842, which is longer than the specified 600\n",
      "Created a chunk of size 1069, which is longer than the specified 600\n",
      "Created a chunk of size 605, which is longer than the specified 600\n",
      "Created a chunk of size 707, which is longer than the specified 600\n",
      "Created a chunk of size 606, which is longer than the specified 600\n",
      "Created a chunk of size 780, which is longer than the specified 600\n",
      "Created a chunk of size 836, which is longer than the specified 600\n",
      "Created a chunk of size 776, which is longer than the specified 600\n",
      "Created a chunk of size 4536, which is longer than the specified 600\n",
      "Created a chunk of size 1134, which is longer than the specified 600\n",
      "Created a chunk of size 822, which is longer than the specified 600\n",
      "Created a chunk of size 1365, which is longer than the specified 600\n",
      "Created a chunk of size 1081, which is longer than the specified 600\n",
      "Created a chunk of size 679, which is longer than the specified 600\n",
      "Created a chunk of size 634, which is longer than the specified 600\n",
      "Created a chunk of size 880, which is longer than the specified 600\n",
      "Created a chunk of size 642, which is longer than the specified 600\n",
      "Created a chunk of size 1208, which is longer than the specified 600\n",
      "Created a chunk of size 609, which is longer than the specified 600\n",
      "Created a chunk of size 612, which is longer than the specified 600\n",
      "Created a chunk of size 1103, which is longer than the specified 600\n",
      "Created a chunk of size 606, which is longer than the specified 600\n",
      "Created a chunk of size 1830, which is longer than the specified 600\n",
      "Created a chunk of size 1787, which is longer than the specified 600\n",
      "Created a chunk of size 692, which is longer than the specified 600\n",
      "Created a chunk of size 695, which is longer than the specified 600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked Document Count: 81077\n"
     ]
    }
   ],
   "source": [
    "#######################\n",
    "## Rag Data Chunking ##\n",
    "#######################\n",
    "loader = DataFrameLoader(rag_data, page_content_column='context')\n",
    "documents = loader.load()\n",
    "\n",
    "# Chunking\n",
    "text_splitter = CharacterTextSplitter.from_tiktoken_encoder(\n",
    "    separator=\". \",\n",
    "    chunk_size=600,\n",
    "    chunk_overlap=200,\n",
    "    encoding_name='cl100k_base'\n",
    ")\n",
    "split_docs = text_splitter.split_documents(tqdm(documents))\n",
    "print(f\"Chunked Document Count: {len(split_docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new vector store...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ingesting documents: 100%|██████████| 81076/81076 [33:34<00:00, 40.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Count in Vector Store: 81077\n"
     ]
    }
   ],
   "source": [
    "###############\n",
    "## Vector DB ##\n",
    "###############\n",
    "model_name = 'dragonkue/BGE-m3-ko'\n",
    "device = 'cuda'\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': device},\n",
    "    encode_kwargs={'normalize_embeddings': True},\n",
    ")\n",
    "\n",
    "# Vector DB path\n",
    "vector_store_path = f\"/data/ephemeral/home/workspace/contest_baseline_code/data/db/faiss_{model_name}_count-{len(split_docs)}\"\n",
    "\n",
    "# Load existing vector store or create new one\n",
    "if os.path.exists(vector_store_path):\n",
    "    print(\"Loading existing vector store...\")\n",
    "    vector_store = FAISS.load_local(\n",
    "        vector_store_path,\n",
    "        embeddings,\n",
    "        allow_dangerous_deserialization=True\n",
    "        )\n",
    "else:\n",
    "    print(\"Creating new vector store...\")\n",
    "    vector_store = FAISS.from_documents(\n",
    "        [split_docs[0]],\n",
    "        embedding=embeddings,\n",
    "        distance_strategy=DistanceStrategy.COSINE\n",
    "    )\n",
    "\n",
    "    # # 인덱스를 GPU로 이동\n",
    "    # res = faiss.StandardGpuResources()\n",
    "    # gpu_index = faiss.index_cpu_to_gpu(res, 0, vector_store.index)  # 0은 GPU ID\n",
    "    # vector_store.index = gpu_index    \n",
    "    \n",
    "    # 배치 크기 설정\n",
    "    batch_size = 4  # 적절한 배치 크기로 조절 가능\n",
    "    docs_to_add = split_docs[1:]\n",
    "    with tqdm(total=len(docs_to_add), desc=\"Ingesting documents\") as pbar:\n",
    "        for i in range(0, len(docs_to_add), batch_size):\n",
    "            batch_docs = docs_to_add[i:i + batch_size]\n",
    "            vector_store.add_documents(batch_docs)\n",
    "            pbar.update(len(batch_docs))\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "    # # 인덱스를 다시 CPU로 이동하여 저장\n",
    "    # cpu_index = faiss.index_gpu_to_cpu(vector_store.index)\n",
    "    # vector_store.index = cpu_index\n",
    "    \n",
    "    # Save vector DB\n",
    "    vector_store.save_local(vector_store_path)\n",
    "\n",
    "# Check the number of documents in the vector store\n",
    "doc_count = vector_store.index.ntotal\n",
    "print(f\"Document Count in Vector Store: {doc_count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Define Retrievers ##\n",
    "#######################\n",
    "\n",
    "# Set top k for retrievers\n",
    "topk=5\n",
    "rerank_topk = 2\n",
    "\n",
    "# Sparse retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(\n",
    "    documents=split_docs,\n",
    "    k=topk,\n",
    "    preprocess_func=extract_nouns,  # MeCab 전처리 함수\n",
    "    # metadata={\"source\": \"faq\", \"version\": \"1.0\"}, # 추가 메타데이터 정보(원본 데이터 뭔지) 출력 가능\n",
    "    # vectorizer=, # 커스텀 벡터라이저 사용 가능\n",
    ")\n",
    "\n",
    "# Dense retriever\n",
    "faiss_retriever = vector_store.as_retriever(search_kwargs={\"k\":topk})\n",
    "\n",
    "# Ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, faiss_retriever], weights=[0.5, 0.5])\n",
    "\n",
    "# Reranker\n",
    "reranker = HuggingFaceCrossEncoder(\n",
    "    model_name=\"dragonkue/bge-reranker-v2-m3-ko\",\n",
    "    model_kwargs={'device': 'cuda'}\n",
    ")\n",
    "\n",
    "# Compressor\n",
    "compressor = CrossEncoderReranker(model=reranker, top_n=rerank_topk)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor, base_retriever=ensemble_retriever\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 869/869 [15:53<00:00,  1.10s/it]\n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "## Save Retrieval Documents ##\n",
    "##############################\n",
    "tqdm.pandas()\n",
    "\n",
    "target_data = pd.read_csv(\"../../contest_baseline_code/data/raw/test.csv\")\n",
    "prompt = \"{paragraph} {question} {choices}\"\n",
    "\n",
    "def process_row(row):\n",
    "    problems = literal_eval(row['problems'])\n",
    "    paragraph = row['paragraph']\n",
    "    question = problems['question']\n",
    "    choices = problems['choices']\n",
    "    choices_str = \" \".join(choices)\n",
    "    \n",
    "    query = prompt.format(paragraph=paragraph, question=question, choices=choices_str)\n",
    "    \n",
    "    if len(paragraph) > 500: # 보수적 기준: 약 600~700자, 널널한 기준: 약 300~400자\n",
    "        return None\n",
    "    docs = compression_retriever.invoke(query)\n",
    "    retrieved_docs = [doc.page_content for doc in docs]\n",
    "    return retrieved_docs\n",
    "    \n",
    "# query 생성\n",
    "target_data['reference'] = target_data.progress_apply(process_row, axis=1)\n",
    "\n",
    "# 결과 저장\n",
    "target_data.to_csv(\"../../contest_baseline_code/data/preprocessed/test_rag.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
