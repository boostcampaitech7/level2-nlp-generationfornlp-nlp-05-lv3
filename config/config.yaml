model:
  experiment_name: &experiment_name "241126_(Qwen_32B)_(train_rag_dense10_v2_list)_(sota_integration_rag)"  # 공통 부분 정의
  train:
    train_model_name: "unsloth/Qwen2.5-32B-Instruct-bnb-4bit" # 학습할 모델 이름
    train_csv_path: "data/rag_results/train_rag_dense10_v2_list.csv" # train csv 파일 경로
    train_checkpoint_path: "checkpoints/{experiment_name}" # 학습한 체크포인트 저장 경로
  test:
    test_checkpoint_path: "checkpoints/{experiment_name}/checkpoint-332" # 추론할 체크포인트 경로
    test_csv_path: "data/rag_results/test_rag_dense10_v2_list.csv" # test 파일 경로
    test_output_csv_path: "data/outputs/{experiment_name}.csv" # 리더보드 제출용 csv 파일 경로

  max_seq_length: 8192
  prompt_name: "SOTA_PROMPT"
  rag: True
  uniform_answer_distribution: False
  train_valid_split: False # True: 0.9 / 0.1

seed: 3407

FastLanguageModel:
  # model_name -> 'train_model_name'로 설정
  # max_seq_length: -> 'max_seq_length'로 설정
  # dtype -> None 하드 코딩
  # load_in_4bit -> True 하드 코딩

peft:
  # model_name -> 'train_model_name'로 설정
  r: 64
  lora_alpha: 32
  lora_dropout: 0
  target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj",]
  bias: "none"
  use_gradient_checkpointing: "unsloth"
  # random_state -> 'seed'로 설정
  use_rslora: True
  # loftq_config -> None 하드 코딩

UnslothTrainingArguments:
  # do_train -> True 하드 코딩
  # do_eval -> 'train_valid_split'에 따라 자동 설정
  per_device_train_batch_size: 2
  per_device_eval_batch_size: 2
  gradient_accumulation_steps: 8
  warmup_ratio: 0.1
  num_train_epochs: 2
  learning_rate: 5e-5
  embedding_learning_rate: 1e-6
  # fp16 -> not is_bfloat16_supported() 하드 코딩
  # bf16 -> is_bfloat16_supported() 하드 코딩
  # logging_steps -> 500 하드 코딩
  optim: "adamw_8bit"
  weight_decay: 0.01
  lr_scheduler_type: "linear"
  # seed -> 'seed'로 설정
  # max_seq_length -> 'max_seq_length'로 설정
  # output_dir -> 'train_checkpoint_path'로 설정
  save_strategy: "epoch"
  eval_strategy: "no"
  save_total_limit: 2
  save_only_model: True
  # report_to -> 'wandb' 하드 코딩